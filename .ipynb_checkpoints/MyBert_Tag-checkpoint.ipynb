{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b44c66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.lang.en import English  # Or whichever language you need\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "import json\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.blank(\"en\")\n",
    "posnlp = spacy.load(\"en_core_web_sm\")\n",
    "db = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb6a5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCBI\n",
    "sys.path.insert(0, 'ncbi-disease/tools/')\n",
    "sys.path.insert(0, 'ncbi-disease/original-data/test/')\n",
    "sys.path.insert(0, 'ncbi-disease/original-data/train/')\n",
    "sys.path.insert(0, 'ncbi-disease/original-data/devel/')\n",
    "\n",
    "from ncbidisease import load_ncbi_disease, read_ncbi_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6887a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "devdata = 'ncbi-disease/original-data/devel/NCBIdevelopset_corpus.txt'\n",
    "traindata = 'ncbi-disease/original-data/train/NCBItrainset_corpus.txt'\n",
    "testdata = 'ncbi-disease/original-data/test/NCBItestset_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa06986",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_documents = load_ncbi_disease(devdata)\n",
    "f_dev = dev_documents\n",
    "train_documents = load_ncbi_disease(traindata)\n",
    "f_train = train_documents\n",
    "test_documents = load_ncbi_disease(testdata)\n",
    "f_test = test_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ccea7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length: Dev, Train, Test 100 593 100\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset length: Dev, Train, Test\", len(f_dev), len(f_train), len(f_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0908028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_input(text, annotation):\n",
    "    results = []\n",
    "    entities = []\n",
    "    for item in annotation:\n",
    "        x = item.split('\\t')\n",
    "        if (x[0][0] != 'N'):\n",
    "            s = x[1].split(' ')\n",
    "            #print(s)\n",
    "            #if ((s[0] == 'DiseaseClass') or (s[0]) == 'SpecificDisease'):\n",
    "            if (s[0] == 'DiseaseClass'):\n",
    "                entities.append((int(s[1]), int(s[2]), s[0]))\n",
    "                if len(entities) > 0:\n",
    "                    results = [text, {\"entities\": entities}]\n",
    "    return (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952f5a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create dataset\n",
    "#\n",
    "def generate_dataset(f):    \n",
    "    DATASET = []\n",
    "    for d in f:\n",
    "        txt = d.tiab # Single text document\n",
    "        txtarr = txt.split(\"\\n\")\n",
    "        text = ' '.join(txtarr)\n",
    "        #print(text)\n",
    "        annotation = d.to_standoff() # Annotations for that document, can contain one or more annotations\n",
    "        results = get_ner_input(text, annotation)\n",
    "        if len(results) == 2: # two elements in list, text + annotation\n",
    "            DATASET.append(results)\n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d4d2b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_DATA = generate_dataset(f_dev)\n",
    "TRAIN_DATA = generate_dataset(f_train)\n",
    "TEST_DATA = generate_dataset(f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e3f23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5331c4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5cf3075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aba88eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(DATA):\n",
    "    \n",
    "    nlp = English()\n",
    "    tokens = []\n",
    "    text_tags = []\n",
    "    text_pos = []\n",
    "    text_sent = []\n",
    "    sent_offset = 0\n",
    "    for text, annotations in DATA:\n",
    "        offsets = annotations[\"entities\"]\n",
    "        doc = nlp(text)\n",
    "        docpos = posnlp(text)\n",
    "        tags = offsets_to_biluo_tags(doc, offsets)\n",
    "        tokens.append([token.text for token in doc])\n",
    "        text_tags.append(tags)                \n",
    "        pos = [token.pos_ for token in docpos]\n",
    "        text_pos.append(pos)\n",
    "        \n",
    "        for sent_i, sent in enumerate(docpos.sents):            \n",
    "            sent_offset += 1\n",
    "            text_sent.append([sent_offset] * len(sent)) # sentence number is continous\n",
    "        #print([token.pos_ for token in docpos])\n",
    "        #print([token.text for token in doc], tags)\n",
    "        \n",
    "    sentences = [item for sublist in text_sent for item in sublist]        \n",
    "    return sentences, tokens, text_pos, text_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9905cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of list to list\n",
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaef0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing code\n",
    "#test_text = [['LEF/1 in the dividing tumour cells beta-catenin/LEF', {'entities': [(22,28,'Disease')]}]]\n",
    "#a = test_text[0][0]\n",
    "#print(a[22:28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22b9cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for text, annotations in test_text:\n",
    "#    doc = nlp(text)\n",
    "#    docpos = posnlp(text)\n",
    "#    offsets = annotations[\"entities\"]\n",
    "#    tags = offsets_to_biluo_tags(doc, offsets)\n",
    "#    pos = [token.pos_ for token in docpos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50c0346c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Heterozygous loss of Six5 in mice is sufficient to...\" with entities \"[(105, 131, 'DiseaseClass'), (175, 183, 'DiseaseCl...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
     ]
    }
   ],
   "source": [
    "sentences, text_tokens, text_pos, text_tags = get_tags(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66875ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75861"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ec0b60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75861"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = flatten(text_tokens)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fee587e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75861"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = flatten(text_tags)\n",
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a90d34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75861"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = flatten(text_pos)\n",
    "len(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a368677",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['sentence#', 'token', 'pos', 'tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b093b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebcb4c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence#'] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75471ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf750051",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tag'] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87d93641",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80f91309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.loc[df['sentence#']==11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06b09072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('NCBITraining_tagged.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b93115ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('NCBITraining_tagged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be3831ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence#</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>DET</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>common</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>human</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>skin</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>B-DiseaseClass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>tumour</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>L-DiseaseClass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence#   token   pos             tag\n",
       "0          1       A   DET               O\n",
       "1          1  common   ADJ               O\n",
       "2          1   human   ADJ               O\n",
       "3          1    skin  NOUN  B-DiseaseClass\n",
       "4          1  tumour  NOUN  L-DiseaseClass"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510656d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
